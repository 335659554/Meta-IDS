F:\anaconda3\envs\py38\lib\site-packages\torch\nn\modules\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Fuzzy1:
Test Loss per batch:3.946093056583777e-05
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:1567.0,FN:0.0
-------------------------------------------
Fuzzy2:
Test Loss per batch:5.9935369790764526e-05
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:3952.0,FN:0.0
-------------------------------------------
Fuzzy3:
Test Loss per batch:6.014808208088983e-05
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:358950.0,FN:0.0
-------------------------------------------
Fuzzy4:
Test Loss per batch:0.005409606266766787
accuracy:0.9987852611688487,recall:0.990280777537797,precision:1.0,f1:0.99511665762344
FPR:0.0,FNR:0.009719222462203024
TP:917.0,FP:0.0,TN:6483.0,FN:9.0
-------------------------------------------
Fuzzy5:
Test Loss per batch:0.003105482763173629
accuracy:0.9992122570281391,recall:0.9938655035576358,precision:0.9998315453971786,f1:0.9968395979318094
FPR:2.392133752931313e-05,FNR:0.006134496442364174
TP:373925.0,FP:63.0,TN:2633569.0,FN:2308.0
-------------------------------------------
Fuzzy6:
Test Loss per batch:0.001958661014214158
accuracy:0.9995609541928875,recall:0.9964871194379391,precision:1.0,f1:0.9982404692082112
FPR:0.0,FNR:0.00351288056206089
TP:851.0,FP:0.0,TN:5979.0,FN:3.0
-------------------------------------------
Fuzzy7:
Test Loss per batch:0.0030960676176272324
accuracy:0.9992142565100136,recall:0.9938779666405062,precision:0.9998351010006371,f1:0.9968476339817833
FPR:2.3416636242779427e-05,FNR:0.006122033359493746
TP:400179.0,FP:66.0,TN:2818443.0,FN:2465.0
-------------------------------------------
Fuzzy8:
Test Loss per batch:8.301315392600372e-05
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:193.0,FP:0.0,TN:1352.0,FN:0.0
-------------------------------------------
Fuzzy9:
Test Loss per batch:0.003101989014507973
accuracy:0.9992126308336174,recall:0.9938696948767782,precision:0.999830338219413,f1:0.9968411061632664
FPR:2.409289301722948e-05,FNR:0.00613030512322181
TP:371264.0,FP:63.0,TN:2614816.0,FN:2290.0
-------------------------------------------
Fuzzy10:
Test Loss per batch:0.003093243257378124
accuracy:0.999214341899956,recall:0.9938778891142392,precision:0.9998358660464929,f1:0.9968479752268979
FPR:2.3307975451323087e-05,FNR:0.006122110885760758
TP:395953.0,FP:65.0,TN:2788680.0,FN:2439.0
-------------------------------------------
