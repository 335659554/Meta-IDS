F:\anaconda3\envs\py38\lib\site-packages\torch\nn\modules\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
gear1:
Test Loss per batch:0.007367624435573816
accuracy:0.9951219512195122,recall:1.0,precision:0.9850746268656716,f1:0.9924812030075187
FPR:0.007194244604316547,FNR:0.0
TP:66.0,FP:1.0,TN:138.0,FN:0.0
-------------------------------------------
gear2:
Test Loss per batch:0.028104323893785477
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:65.0,FP:0.0,TN:140.0,FN:0.0
-------------------------------------------
gear3:
Test Loss per batch:0.003372546285390854
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:64.0,FP:0.0,TN:141.0,FN:0.0
-------------------------------------------
gear4:
Test Loss per batch:0.07804551720619202
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:63.0,FP:0.0,TN:142.0,FN:0.0
-------------------------------------------
gear5:
Test Loss per batch:0.0023683635517954826
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:62.0,FP:0.0,TN:143.0,FN:0.0
-------------------------------------------
gear6:
Test Loss per batch:0.058242082595825195
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:63.0,FP:0.0,TN:142.0,FN:0.0
-------------------------------------------
gear7:
Test Loss per batch:0.0029442450031638145
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:67.0,FP:0.0,TN:138.0,FN:0.0
-------------------------------------------
gear10:
Test Loss per batch:0.004577918443828821
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:63.0,FP:0.0,TN:142.0,FN:0.0
-------------------------------------------
