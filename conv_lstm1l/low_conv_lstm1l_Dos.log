F:\anaconda3\envs\py38\lib\site-packages\torch\nn\modules\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Dos1:
Test Loss per batch:0.004667982924729586
accuracy:0.998645903859174,recall:1.0,precision:0.3333333333333333,f1:0.5
FPR:0.0013550135501355014,FNR:0.0
TP:1.0,FP:2.0,TN:1474.0,FN:0.0
-------------------------------------------
Dos2:
Test Loss per batch:0.004203415475785732
accuracy:0.998776758409786,recall:1.0,precision:0.9903069466882067,f1:0.9951298701298701
FPR:0.0013979496738117428,FNR:0.0
TP:613.0,FP:6.0,TN:4286.0,FN:0.0
-------------------------------------------
Dos3:
Test Loss per batch:0.0053174677304923534
accuracy:0.9984975961538461,recall:1.0,precision:0.16666666666666666,f1:0.2857142857142857
FPR:0.0015028554253080854,FNR:0.0
TP:1.0,FP:5.0,TN:3322.0,FN:0.0
-------------------------------------------
Dos4:
Test Loss per batch:0.005451457109302282
accuracy:0.997275204359673,recall:1.0,precision:0.5,f1:0.6666666666666666
FPR:0.00273224043715847,FNR:0.0
TP:1.0,FP:1.0,TN:365.0,FN:0.0
-------------------------------------------
Dos5:
Test Loss per batch:0.004062703810632229
accuracy:0.9988191300925015,recall:1.0,precision:0.9906396255850234,f1:0.9952978056426333
FPR:0.001349527665317139,FNR:0.0
TP:635.0,FP:6.0,TN:4440.0,FN:0.0
-------------------------------------------
Dos6:
Test Loss per batch:0.0038254926117455086
accuracy:0.9989253422264283,recall:1.0,precision:0.9914760143183804,f1:0.9957197648275281
FPR:0.0012281801802423414,FNR:0.0
TP:165634.0,FP:1424.0,TN:1158015.0,FN:0.0
-------------------------------------------
Dos7:
Test Loss per batch:0.004586936440318823
accuracy:0.998727411555103,recall:1.0,precision:0.16666666666666666,f1:0.2857142857142857
FPR:0.0012729124236252546,FNR:0.0
TP:1.0,FP:5.0,TN:3923.0,FN:0.0
-------------------------------------------
Dos8:
Test Loss per batch:0.003791925887907705
accuracy:0.9989265409767187,recall:1.0,precision:0.9914854438074703,f1:0.9957245199963647
FPR:0.00122681022538386,FNR:0.0
TP:251989.0,FP:2164.0,TN:1761760.0,FN:0.0
-------------------------------------------
Dos9:
Test Loss per batch:0.005499126389622688
accuracy:0.9984639016897081,recall:1.0,precision:0.3333333333333333,f1:0.5
FPR:0.0015372790161414297,FNR:0.0
TP:1.0,FP:2.0,TN:1299.0,FN:0.0
-------------------------------------------
Dos10:
Test Loss per batch:0.0037924253715659086
accuracy:0.9989259967891109,recall:1.0,precision:0.9914811633898934,f1:0.9957223614429745
FPR:0.0012274321383874552,FNR:0.0
TP:213570.0,FP:1835.0,TN:1493156.0,FN:0.0
-------------------------------------------
