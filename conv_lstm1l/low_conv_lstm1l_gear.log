F:\anaconda3\envs\py38\lib\site-packages\torch\nn\modules\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
gear1:
Test Loss per batch:5.707243417418795e-06
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:2140.0,FN:0.0
-------------------------------------------
gear2:
Test Loss per batch:3.208760972484015e-05
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:479.0,FN:0.0
-------------------------------------------
gear3:
Test Loss per batch:0.00015946094774537615
accuracy:0.9999017207066281,recall:1.0,precision:0.011764705882352941,f1:0.023255813953488372
FPR:9.827940835796169e-05,FNR:0.0
TP:1.0,FP:84.0,TN:854622.0,FN:0.0
-------------------------------------------
gear4:
Test Loss per batch:0.0032793890027438895
accuracy:0.9991369176592657,recall:1.0,precision:0.9931426862031076,f1:0.9965595469685337
FPR:0.00098637976696778,FNR:0.0
TP:345274.0,FP:2384.0,TN:2414535.0,FN:0.0
-------------------------------------------
gear5:
Test Loss per batch:0.004613822791725397
accuracy:0.99866325052913,recall:1.0,precision:0.9894179894179894,f1:0.9946808510638298
FPR:0.001527689369828135,FNR:0.0
TP:1122.0,FP:12.0,TN:7843.0,FN:0.0
-------------------------------------------
gear6:
Test Loss per batch:0.0015085499035194516
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:33.0,FP:0.0,TN:232.0,FN:0.0
-------------------------------------------
gear7:
Test Loss per batch:0.002642205450683832
accuracy:0.9994169096209913,recall:1.0,precision:0.9953560371517027,f1:0.9976726144297905
FPR:0.0006663705019991116,FNR:0.0
TP:643.0,FP:3.0,TN:4499.0,FN:0.0
-------------------------------------------
gear8:
Test Loss per batch:1.5473828170797788e-05
accuracy:0.999995646722853,recall:1.0,precision:0.5,f1:0.6666666666666666
FPR:4.353296098140707e-06,FNR:0.0
TP:1.0,FP:1.0,TN:229710.0,FN:0.0
-------------------------------------------
gear9:
Test Loss per batch:1.956436108230264e-06
accuracy:1.0,recall:1.0,precision:1.0,f1:1.0
FPR:0.0,FNR:0.0
TP:1.0,FP:0.0,TN:6341.0,FN:0.0
-------------------------------------------
gear10:
Test Loss per batch:0.00025740488698439937
accuracy:0.9998493762404309,recall:1.0,precision:0.0072992700729927005,f1:0.014492753623188406
FPR:0.00015062392638920115,FNR:0.0
TP:1.0,FP:136.0,TN:902775.0,FN:0.0
-------------------------------------------
