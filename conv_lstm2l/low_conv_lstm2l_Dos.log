F:\anaconda3\envs\py38\lib\site-packages\torch\nn\modules\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\cudnn\RNN.cpp:982.)
  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
Dos1:
Test Loss per batch:0.003694943618029356
accuracy:0.998645903859174,recall:1.0,precision:0.3333333333333333,f1:0.5
FPR:0.0013550135501355014,FNR:0.0
TP:1.0,FP:2.0,TN:1474.0,FN:0.0
-------------------------------------------
Dos2:
Test Loss per batch:0.004202876705676317
accuracy:0.998776758409786,recall:1.0,precision:0.9903069466882067,f1:0.9951298701298701
FPR:0.0013979496738117428,FNR:0.0
TP:613.0,FP:6.0,TN:4286.0,FN:0.0
-------------------------------------------
Dos3:
Test Loss per batch:0.004046967718750238
accuracy:0.9984975961538461,recall:1.0,precision:0.16666666666666666,f1:0.2857142857142857
FPR:0.0015028554253080854,FNR:0.0
TP:1.0,FP:5.0,TN:3322.0,FN:0.0
-------------------------------------------
Dos4:
Test Loss per batch:0.0022836660500615835
accuracy:0.997275204359673,recall:1.0,precision:0.5,f1:0.6666666666666666
FPR:0.00273224043715847,FNR:0.0
TP:1.0,FP:1.0,TN:365.0,FN:0.0
-------------------------------------------
Dos5:
Test Loss per batch:0.004124315921217203
accuracy:0.9988191300925015,recall:1.0,precision:0.9906396255850234,f1:0.9952978056426333
FPR:0.001349527665317139,FNR:0.0
TP:635.0,FP:6.0,TN:4440.0,FN:0.0
-------------------------------------------
Dos6:
Test Loss per batch:0.003947965982483655
accuracy:0.9989230782002199,recall:0.9999818877766642,precision:0.9914758612433031,f1:0.9957107088001106
FPR:0.0012281801802423414,FNR:1.8112223335788545e-05
TP:165631.0,FP:1424.0,TN:1158015.0,FN:3.0
-------------------------------------------
Dos7:
Test Loss per batch:0.003581717610359192
accuracy:0.998727411555103,recall:1.0,precision:0.16666666666666666,f1:0.2857142857142857
FPR:0.0012729124236252546,FNR:0.0
TP:1.0,FP:5.0,TN:3923.0,FN:0.0
-------------------------------------------
Dos8:
Test Loss per batch:0.003925162217309398
accuracy:0.9989225725514941,recall:0.999968252582454,precision:0.9914851757854768,f1:0.9957086463268621
FPR:0.00122681022538386,FNR:3.174741754600399e-05
TP:251981.0,FP:2164.0,TN:1761760.0,FN:8.0
-------------------------------------------
Dos9:
Test Loss per batch:0.004268142860382795
accuracy:0.9984639016897081,recall:1.0,precision:0.3333333333333333,f1:0.5
FPR:0.0015372790161414297,FNR:0.0
TP:1.0,FP:2.0,TN:1299.0,FN:0.0
-------------------------------------------
Dos10:
Test Loss per batch:0.003917160742687729
accuracy:0.9989213144862841,recall:0.9999625415554619,precision:0.991480846994155,f1:0.995703632214133
FPR:0.0012274321383874552,FNR:3.7458444538090555e-05
TP:213562.0,FP:1835.0,TN:1493156.0,FN:8.0
-------------------------------------------
